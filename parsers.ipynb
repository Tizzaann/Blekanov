{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4 lxml langchain pymupdf docx2txt pywin32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    text = soup.get_text(separator=' ')\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сайт начинающего верстальщика. Блог. На главную. День двенадцатый. Все любят печенье. Сегодня Кекс попросил меня сделать рецепт того самого рыбного печенья доступным для всех своих клиентов. Но вот незадача, у меня нет принтера, да и печатать рецепт на всех, тратить бумагу, не хочется. Пришлось сделать ссылку для скачивания файла. Скачать. Тут могла быть ваша реклама. Подвал сайта.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'docs/keks.html'\n",
    "text = extract_text_from_html(file_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.02.202 5 ПРАКТИЧЕСКИЕ ЗАДАНИЯ 1. МОДУЛЬ 1 ( 17.02.202 5-17.03.202 5) Разработка парсер -менеджера A. Проектно -практическое задание модуля i. Разработка и тестирование синтаксического анализатора для обработки html страниц. ii. Разработка и тестирование синтаксического анализатора для автоматизированной обработки документов форматов .pdf, .doc, . docx , djvu . B. Темы семинаров модуля: i. Современные методы ранжирования результатов информационного поиска в Вебе. ii. Mining query logs to improve web search engines' operations. iii. Модели языка (Bag of words, Word2Vec, Word embedding и др.) и методы обработки текстово й информации (WordNet, лемматизация , стемминг ). iv. Кодирование текстовых данных на основе модели GPT . v. Выявление именованных сущностей (NER). C. Deadline модуля – 17.03.202 3 2. МОДУЛЬ 2 (18.03.202 5-21.04.202 5) Разработка поискового робота для сбора и обработки данных с ресурсов Web 1.0/ Web 2.0 A. Проектно -практическое задание модуля i. Разработка простейшей модели поискового робота с классическим алгоритмом сбора и обработки данных в сети Веб 1.0/ Веб 2.0. ii. Автоматизированны й сбор данных с помощью простейшей модели поискового робота на основе специализированного алгоритма обхода на примере сайтов СПбГУ и МГУ – для Веб 1.0 / Автоматизированный сбор данных на основе API социальной сети VKontakte или мессенджера Телеграм об упоминаниях в пользовательских публикациях СПбГУ и МГУ – для Веб 2.0. iii. Сбор статистики обработанных страниц для Веб 1.0 : общее количество страниц и всех ссылок, количество внутренних страниц, количество неработающих страниц, количество внутренних поддоменов, общее количество ссылок на внешние ресурсы , количество уникальных внешних ресурсов, количество уникальных ссылок на файлы doc/docx /pdf. Статистика для Веб 2.0: количество публикаций об упоминании университета, количество публикующих контент пользователей, количество лайков/просмотров/комментариев/репостов, график количество публикаций в день за собираемый период. B. Темы семинаров модуля: i. Методы обновления данных в индексе с помощью поисковых роботов. Стратегии равномерного и пропорционального обновления. ii. Алгоритм PageRank и его модификации для вычисления весов Веб - страниц сайтов. iii. Меры центральности графов, используемые при анализе данных социальных сете й. iv. Обучение с подкреплением (Reinforcement Learning) . v. Методы сокращения размерности данных. C. Deadline модуля – 21.04.202 5 3. МОДУЛЬ 3 ( 22.04.202 5-22.05.202 5) Разработка простейшей модели инвертированного индекса A. Проектно -практическое задание модуля i. Реализация и тестирование индексной структуры на основе инвертированного индекса. ii. Применение метода сжатия инвертированного индекса с использованием дельта и гамма -кодирования Элиаса. iii. Тестирование процесса индексирования собранных на 2м этапе веб - страниц сайта (упоминаний в социальной сети VK) СПбГУ или МГУ (скорость процесса индексирования по количеству текстовых документов около 40 тыс., проверить на сколько эффективно индексирование с использованием алгоритма сжатия уменьшает объемы хранимой информации по сравнению с классической ситуацией, не предусматривающей использование алгоритма сжатия). Проверить скорость поиска по запросу «Ректор СПбГУ /МГУ». B. Темы семинаров модуля: i. Распределенное (MapReduce) и динамическое индексирование. ii. Алгоритмы сжатия: непараметрические алгоритмы дельта - и гамма - кодирования Элиаса. Алгоритмы сжатия: параметрический алгоритм кодирования Голомба. iii. Индексная структура суффиксные деревья, их принципы структуризации информации и архитектурные особенности. iv. Задача дедупликации данных C. Deadline модуля – 26.05.202 5\n"
     ]
    }
   ],
   "source": [
    "file_path = 'docs/praktika.pdf'\n",
    "text = extract_text_from_pdf(file_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(file_path):\n",
    "    loader = Docx2txtLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text = \" \".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.02.2025 Практические задания Модуль 1 (17.02.2025-17.03.2025) Разработка парсер-менеджера Проектно-практическое задание модуля Разработка и тестирование синтаксического анализатора для обработки html страниц. Разработка и тестирование синтаксического анализатора для автоматизированной обработки документов форматов .pdf, .doc, .docx, djvu. Темы семинаров модуля: Современные методы ранжирования результатов информационного поиска в Вебе. Mining query logs to improve web search engines' operations. Модели языка (Bag of words, Word2Vec, Word embedding и др.) и методы обработки текстовой информации (WordNet, лемматизация, стемминг). Кодирование текстовых данных на основе модели GPT. Выявление именованных сущностей (NER). Deadline модуля – 17.03.2023 Модуль 2 (18.03.2025-21.04.2025) Разработка поискового робота для сбора и обработки данных с ресурсов Web 1.0/Web 2.0 Проектно-практическое задание модуля Разработка простейшей модели поискового робота с классическим алгоритмом сбора и обработки данных в сети Веб 1.0/Веб 2.0. Автоматизированный сбор данных с помощью простейшей модели поискового робота на основе специализированного алгоритма обхода на примере сайтов СПбГУ и МГУ – для Веб 1.0 / Автоматизированный сбор данных на основе API социальной сети VKontakte или мессенджера Телеграм об упоминаниях в пользовательских публикациях СПбГУ и МГУ – для Веб 2.0. Сбор статистики обработанных страниц для Веб 1.0: общее количество страниц и всех ссылок, количество внутренних страниц, количество неработающих страниц, количество внутренних поддоменов, общее количество ссылок на внешние ресурсы, количество уникальных внешних ресурсов, количество уникальных ссылок на файлы doc/docx/pdf. Статистика для Веб 2.0: количество публикаций об упоминании университета, количество публикующих контент пользователей, количество лайков/просмотров/комментариев/репостов, график количество публикаций в день за собираемый период. Темы семинаров модуля: Методы обновления данных в индексе с помощью поисковых роботов. Стратегии равномерного и пропорционального обновления. Алгоритм PageRank и его модификации для вычисления весов Веб-страниц сайтов. Меры центральности графов, используемые при анализе данных социальных сетей. Обучение с подкреплением (Reinforcement Learning). Методы сокращения размерности данных. Deadline модуля – 21.04.2025 Модуль 3 (22.04.2025-22.05.2025) Разработка простейшей модели инвертированного индекса Проектно-практическое задание модуля Реализация и тестирование индексной структуры на основе инвертированного индекса. Применение метода сжатия инвертированного индекса с использованием дельта и гамма-кодирования Элиаса. Тестирование процесса индексирования собранных на 2м этапе веб-страниц сайта (упоминаний в социальной сети VK) СПбГУ или МГУ (скорость процесса индексирования по количеству текстовых документов около 40 тыс., проверить на сколько эффективно индексирование с использованием алгоритма сжатия уменьшает объемы хранимой информации по сравнению с классической ситуацией, не предусматривающей использование алгоритма сжатия). Проверить скорость поиска по запросу «Ректор СПбГУ/МГУ». Темы семинаров модуля: Распределенное (MapReduce) и динамическое индексирование. Алгоритмы сжатия: непараметрические алгоритмы дельта- и гамма-кодирования Элиаса. Алгоритмы сжатия: параметрический алгоритм кодирования Голомба. Индексная структура суффиксные деревья, их принципы структуризации информации и архитектурные особенности. Задача дедупликации данных Deadline модуля – 26.05.2025\n"
     ]
    }
   ],
   "source": [
    "file_path = 'docs/praktika.docx'\n",
    "text = extract_text_from_docx(file_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document(file_path):\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    extension = extension.lower()\n",
    "\n",
    "    if extension == '.html':\n",
    "        return extract_text_from_html(file_path)\n",
    "    elif extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif extension == '.docx':\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Неподдерживаемый формат файла: {extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    links = url_pattern.findall(text)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(folder_path):\n",
    "    documents_text = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if filename.lower().endswith(('.html', '.pdf', '.docx')):\n",
    "            try:\n",
    "                print(f\"Обработка файла: {filename}\")\n",
    "                text = parse_document(file_path)\n",
    "                links = find_links(text)\n",
    "                documents_text[filename] = (text, links)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке файла {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"Файл {filename} пропущен (неподдерживаемый формат).\")\n",
    "\n",
    "    return documents_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка файла: keks.html\n",
      "Обработка файла: praktika.docx\n",
      "Обработка файла: praktika.pdf\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'docs'\n",
    "all_documents = process_documents(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл: keks.html\n",
      "Текст: Сайт начинающего верстальщика. Блог. На главную. День двенадцатый. Все любят печенье. Сегодня Кекс попросил меня сделать рецепт того самого рыбного печенья доступным для всех своих клиентов. Но вот незадача, у меня нет принтера, да и печатать рецепт на всех, тратить бумагу, не хочется. Пришлось сделать ссылку для скачивания файла. Скачать. Тут могла быть ваша реклама. Подвал сайта.\n",
      "Ссылки: []\n",
      "--------------------------------------------------\n",
      "Файл: praktika.docx\n",
      "Текст: 17.02.2025 Практические задания Модуль 1 (17.02.2025-17.03.2025) Разработка парсер-менеджера Проектно-практическое задание модуля Разработка и тестирование синтаксического анализатора для обработки html страниц. Разработка и тестирование синтаксического анализатора для автоматизированной обработки документов форматов .pdf, .doc, .docx, djvu. Темы семинаров модуля: Современные методы ранжирования результатов информационного поиска в Вебе. Mining query logs to improve web search engines' operations. Модели языка (Bag of words, Word2Vec, Word embedding и др.) и методы обработки текстовой информации (WordNet, лемматизация, стемминг). Кодирование текстовых данных на основе модели GPT. Выявление именованных сущностей (NER). Deadline модуля – 17.03.2023 Модуль 2 (18.03.2025-21.04.2025) Разработка поискового робота для сбора и обработки данных с ресурсов Web 1.0/Web 2.0 Проектно-практическое задание модуля Разработка простейшей модели поискового робота с классическим алгоритмом сбора и обработки данных в сети Веб 1.0/Веб 2.0. Автоматизированный сбор данных с помощью простейшей модели поискового робота на основе специализированного алгоритма обхода на примере сайтов СПбГУ и МГУ – для Веб 1.0 / Автоматизированный сбор данных на основе API социальной сети VKontakte или мессенджера Телеграм об упоминаниях в пользовательских публикациях СПбГУ и МГУ – для Веб 2.0. Сбор статистики обработанных страниц для Веб 1.0: общее количество страниц и всех ссылок, количество внутренних страниц, количество неработающих страниц, количество внутренних поддоменов, общее количество ссылок на внешние ресурсы, количество уникальных внешних ресурсов, количество уникальных ссылок на файлы doc/docx/pdf. Статистика для Веб 2.0: количество публикаций об упоминании университета, количество публикующих контент пользователей, количество лайков/просмотров/комментариев/репостов, график количество публикаций в день за собираемый период. Темы семинаров модуля: Методы обновления данных в индексе с помощью поисковых роботов. Стратегии равномерного и пропорционального обновления. Алгоритм PageRank и его модификации для вычисления весов Веб-страниц сайтов. Меры центральности графов, используемые при анализе данных социальных сетей. Обучение с подкреплением (Reinforcement Learning). Методы сокращения размерности данных. Deadline модуля – 21.04.2025 Модуль 3 (22.04.2025-22.05.2025) Разработка простейшей модели инвертированного индекса Проектно-практическое задание модуля Реализация и тестирование индексной структуры на основе инвертированного индекса. Применение метода сжатия инвертированного индекса с использованием дельта и гамма-кодирования Элиаса. Тестирование процесса индексирования собранных на 2м этапе веб-страниц сайта (упоминаний в социальной сети VK) СПбГУ или МГУ (скорость процесса индексирования по количеству текстовых документов около 40 тыс., проверить на сколько эффективно индексирование с использованием алгоритма сжатия уменьшает объемы хранимой информации по сравнению с классической ситуацией, не предусматривающей использование алгоритма сжатия). Проверить скорость поиска по запросу «Ректор СПбГУ/МГУ». Темы семинаров модуля: Распределенное (MapReduce) и динамическое индексирование. Алгоритмы сжатия: непараметрические алгоритмы дельта- и гамма-кодирования Элиаса. Алгоритмы сжатия: параметрический алгоритм кодирования Голомба. Индексная структура суффиксные деревья, их принципы структуризации информации и архитектурные особенности. Задача дедупликации данных Deadline модуля – 26.05.2025\n",
      "Ссылки: []\n",
      "--------------------------------------------------\n",
      "Файл: praktika.pdf\n",
      "Текст: 17.02.202 5 ПРАКТИЧЕСКИЕ ЗАДАНИЯ 1. МОДУЛЬ 1 ( 17.02.202 5-17.03.202 5) Разработка парсер -менеджера A. Проектно -практическое задание модуля i. Разработка и тестирование синтаксического анализатора для обработки html страниц. ii. Разработка и тестирование синтаксического анализатора для автоматизированной обработки документов форматов .pdf, .doc, . docx , djvu . B. Темы семинаров модуля: i. Современные методы ранжирования результатов информационного поиска в Вебе. ii. Mining query logs to improve web search engines' operations. iii. Модели языка (Bag of words, Word2Vec, Word embedding и др.) и методы обработки текстово й информации (WordNet, лемматизация , стемминг ). iv. Кодирование текстовых данных на основе модели GPT . v. Выявление именованных сущностей (NER). C. Deadline модуля – 17.03.202 3 2. МОДУЛЬ 2 (18.03.202 5-21.04.202 5) Разработка поискового робота для сбора и обработки данных с ресурсов Web 1.0/ Web 2.0 A. Проектно -практическое задание модуля i. Разработка простейшей модели поискового робота с классическим алгоритмом сбора и обработки данных в сети Веб 1.0/ Веб 2.0. ii. Автоматизированны й сбор данных с помощью простейшей модели поискового робота на основе специализированного алгоритма обхода на примере сайтов СПбГУ и МГУ – для Веб 1.0 / Автоматизированный сбор данных на основе API социальной сети VKontakte или мессенджера Телеграм об упоминаниях в пользовательских публикациях СПбГУ и МГУ – для Веб 2.0. iii. Сбор статистики обработанных страниц для Веб 1.0 : общее количество страниц и всех ссылок, количество внутренних страниц, количество неработающих страниц, количество внутренних поддоменов, общее количество ссылок на внешние ресурсы , количество уникальных внешних ресурсов, количество уникальных ссылок на файлы doc/docx /pdf. Статистика для Веб 2.0: количество публикаций об упоминании университета, количество публикующих контент пользователей, количество лайков/просмотров/комментариев/репостов, график количество публикаций в день за собираемый период. B. Темы семинаров модуля: i. Методы обновления данных в индексе с помощью поисковых роботов. Стратегии равномерного и пропорционального обновления. ii. Алгоритм PageRank и его модификации для вычисления весов Веб - страниц сайтов. iii. Меры центральности графов, используемые при анализе данных социальных сете й. iv. Обучение с подкреплением (Reinforcement Learning) . v. Методы сокращения размерности данных. C. Deadline модуля – 21.04.202 5 3. МОДУЛЬ 3 ( 22.04.202 5-22.05.202 5) Разработка простейшей модели инвертированного индекса A. Проектно -практическое задание модуля i. Реализация и тестирование индексной структуры на основе инвертированного индекса. ii. Применение метода сжатия инвертированного индекса с использованием дельта и гамма -кодирования Элиаса. iii. Тестирование процесса индексирования собранных на 2м этапе веб - страниц сайта (упоминаний в социальной сети VK) СПбГУ или МГУ (скорость процесса индексирования по количеству текстовых документов около 40 тыс., проверить на сколько эффективно индексирование с использованием алгоритма сжатия уменьшает объемы хранимой информации по сравнению с классической ситуацией, не предусматривающей использование алгоритма сжатия). Проверить скорость поиска по запросу «Ректор СПбГУ /МГУ». B. Темы семинаров модуля: i. Распределенное (MapReduce) и динамическое индексирование. ii. Алгоритмы сжатия: непараметрические алгоритмы дельта - и гамма - кодирования Элиаса. Алгоритмы сжатия: параметрический алгоритм кодирования Голомба. iii. Индексная структура суффиксные деревья, их принципы структуризации информации и архитектурные особенности. iv. Задача дедупликации данных C. Deadline модуля – 26.05.202 5\n",
      "Ссылки: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for filename, text in all_documents.items():\n",
    "    print(f\"Файл: {filename}\")\n",
    "    print(f\"Текст: {text[0]}\")\n",
    "    print(f\"Ссылки: {text[1]}\") \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
